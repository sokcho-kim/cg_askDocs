"""
RAG ìµœì í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
í†µì¼ëœ ì²­í¬ í˜•ì‹, í’ˆì§ˆ ì ìˆ˜, í‚¤ì›Œë“œ ì¶”ì¶œ, í–¥ìƒëœ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from utils.chunk_processor import ChunkProcessor, ExcelChunkProcessor
from rag.retriever import EnhancedRetriever


def test_chunk_processor():
    """ì²­í¬ í”„ë¡œì„¸ì„œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("=" * 50)
    print("ğŸ§ª ì²­í¬ í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ìš© í”„ë¡œì„¸ì„œ ìƒì„±
    processor = ChunkProcessor(document_id="test_doc")
    
    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_text = "ìŠ¤ë§ˆíŠ¸ ì•¼ë“œëŠ” 4ì°¨ ì‚°ì—…í˜ëª… ê¸°ìˆ ì„ ìœµí•©í•˜ì—¬ ì§€ëŠ¥í™”ëœ ìƒì‚° í™˜ê²½ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. AI, IoT, ë¹…ë°ì´í„° ê¸°ìˆ ì´ í•µì‹¬ì…ë‹ˆë‹¤."
    
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸: {test_text}")
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    keywords = processor.extract_keywords(test_text)
    print(f"ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
    
    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸
    quality_score = processor.calculate_chunk_quality_score(test_text, "text")
    print(f"ğŸ“Š í’ˆì§ˆ ì ìˆ˜: {quality_score:.3f}")
    
    # ì²­í¬ ìƒì„± í…ŒìŠ¤íŠ¸
    chunk = processor.create_text_chunk(test_text, "page:1")
    print(f"ğŸ“¦ ìƒì„±ëœ ì²­í¬:")
    print(f"  - ID: {chunk['chunk_id']}")
    print(f"  - íƒ€ì…: {chunk['chunk_type']}")
    print(f"  - í’ˆì§ˆ ì ìˆ˜: {chunk['metadata']['quality_score']:.3f}")
    print(f"  - í‚¤ì›Œë“œ: {chunk['metadata']['keywords']}")
    print(f"  - ìš°ì„ ìˆœìœ„: {chunk['metadata']['search_priority']}")
    
    return chunk


def test_excel_processor():
    """Excel í”„ë¡œì„¸ì„œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 50)
    print("ğŸ“Š Excel í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ìš© Excel ë°ì´í„°
    test_data = [
        {
            "ì£¼ì°¨": "2452",
            "ëŒ€ë¶„ë¥˜": "ì‚¬ì™¸",
            "íŒ€": "ì‚¬ì™¸ê³µì •ê´€ë¦¬íŒ€",
            "ì´ìŠˆ": "ì‚¬ì™¸ë¸”ë¡ 2ê°œ ì…ê³  ì§€ì—°",
            "ë¦¬ìŠ¤í¬": "ê³µì • ì§€ì—°"
        },
        {
            "ì£¼ì°¨": "2451",
            "ëŒ€ë¶„ë¥˜": "ìì¬",
            "íŒ€": "ë°°ê´€ì¬êµ¬ë§¤íŒ€",
            "ì´ìŠˆ": "ë°¸ë¸Œì¬ ë³´ê¸‰ì¶©ì¡±ë¥  ì €ì¡°",
            "ë¦¬ìŠ¤í¬": "ê³µì • ì§€ì—°"
        }
    ]
    
    # Excel í”„ë¡œì„¸ì„œ ìƒì„±
    processor = ExcelChunkProcessor(document_id="test_excel")
    
    # Excel ë°ì´í„° ì²˜ë¦¬
    chunks = processor.process_excel_data(test_data, "Sheet1")
    
    print(f"ğŸ“‹ ì²˜ë¦¬ëœ Excel ì²­í¬ ìˆ˜: {len(chunks)}")
    
    for i, chunk in enumerate(chunks, 1):
        print(f"\nğŸ“¦ Excel ì²­í¬ {i}:")
        print(f"  - ID: {chunk['chunk_id']}")
        print(f"  - ìœ„ì¹˜: {chunk['location']}")
        print(f"  - ë‚´ìš©: {chunk['content'][:100]}...")
        print(f"  - í’ˆì§ˆ ì ìˆ˜: {chunk['metadata']['quality_score']:.3f}")
        print(f"  - í‚¤ì›Œë“œ: {chunk['metadata']['keywords'][:5]}")
    
    return chunks


def test_enhanced_retriever():
    """í–¥ìƒëœ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 50)
    print("ğŸ” í–¥ìƒëœ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # Retriever ì´ˆê¸°í™”
    retriever = EnhancedRetriever()
    
    # í…ŒìŠ¤íŠ¸ìš© ì²­í¬ë“¤ ìƒì„±
    test_chunks = [
        {
            "chunk_id": "test_1",
            "content": "ìŠ¤ë§ˆíŠ¸ ì•¼ë“œëŠ” 4ì°¨ ì‚°ì—…í˜ëª… ê¸°ìˆ ì„ ìœµí•©í•œ ì§€ëŠ¥í™”ëœ ì¡°ì„ ì†Œì…ë‹ˆë‹¤.",
            "metadata": {
                "quality_score": 0.8,
                "keywords": ["ìŠ¤ë§ˆíŠ¸", "ì•¼ë“œ", "4ì°¨", "ì‚°ì—…í˜ëª…", "ê¸°ìˆ ", "ì§€ëŠ¥í™”", "ì¡°ì„ ì†Œ"],
                "search_priority": "high"
            }
        },
        {
            "chunk_id": "test_2", 
            "content": "AIì™€ IoT ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ìƒì‚°ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.",
            "metadata": {
                "quality_score": 0.7,
                "keywords": ["AI", "IoT", "ê¸°ìˆ ", "ìƒì‚°ì„±", "í–¥ìƒ"],
                "search_priority": "medium"
            }
        },
        {
            "chunk_id": "test_3",
            "content": "ìë™í™” ì‹œìŠ¤í…œìœ¼ë¡œ ì•ˆì „ì„±ì„ ê°•í™”í•©ë‹ˆë‹¤.",
            "metadata": {
                "quality_score": 0.6,
                "keywords": ["ìë™í™”", "ì‹œìŠ¤í…œ", "ì•ˆì „ì„±", "ê°•í™”"],
                "search_priority": "medium"
            }
        }
    ]
    
    # ì²­í¬ë“¤ì„ ë²¡í„° DBì— ì¶”ê°€
    print("ğŸ“¥ í…ŒìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ë²¡í„° DBì— ì¶”ê°€ ì¤‘...")
    retriever.add_chunks(test_chunks)
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    test_queries = [
        "ìŠ¤ë§ˆíŠ¸ ì•¼ë“œ ìë™í™”",
        "AI ê¸°ìˆ  í™œìš©",
        "ìƒì‚°ì„± í–¥ìƒ"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        results = retriever.hybrid_search(query, 2)
        print(f"  í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼:")
        for i, result in enumerate(results, 1):
            print(f"    {i}. {result['chunk_id']} (ì ìˆ˜: {result['final_score']:.3f})")
        
        # í’ˆì§ˆ í•„í„°ë§ ê²€ìƒ‰
        results = retriever.quality_filtered_search(query, 2, min_quality_score=0.6)
        print(f"  í’ˆì§ˆ í•„í„°ë§ ê²€ìƒ‰ ê²°ê³¼:")
        for i, result in enumerate(results, 1):
            print(f"    {i}. {result['chunk_id']} (í’ˆì§ˆ: {result['metadata']['quality_score']:.2f})")
    
    # í†µê³„ ì •ë³´
    stats = retriever.get_collection_stats()
    print(f"\nğŸ“Š ì»¬ë ‰ì…˜ í†µê³„: {stats}")
    
    return retriever


def test_format_unification():
    """í˜•ì‹ í†µì¼ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 50)
    print("ğŸ”„ í˜•ì‹ í†µì¼ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ê¸°ì¡´ í˜•ì‹ì˜ ì²­í¬ë“¤ (PDFì™€ Excel)
    pdf_chunk = {
        "chunk_id": "smart_yard_intro_text_0",
        "document_id": "smart_yard_intro",
        "chunk_index": 0,
        "chunk_type": "text",
        "location": "page:1",
        "content": "Part ï¼‘ì¡°ì„  ì‚°ì—…ì˜ í˜„í™© ë° ë„ì „ ê³¼ì œ...",
        "embedding": None,
        "metadata": {
            "length": 361,
            "chunk_in_content": 0,
            "page": 1
        }
    }
    
    excel_chunk = {
        "chunk_id": "bfc7d58f-4bba-43da-9fc9-7e1379ce0a75",
        "row_index": 0,
        "column": "ì£¼ì°¨",
        "content": "ì£¼ì°¨: 2452",
        "metadata": {
            "source_file": "{df}",
            "length": 8
        }
    }
    
    print("ğŸ“„ ê¸°ì¡´ PDF ì²­í¬ í˜•ì‹:")
    print(f"  - í•„ë“œ ìˆ˜: {len(pdf_chunk)}")
    print(f"  - í•„ë“œ: {list(pdf_chunk.keys())}")
    
    print("\nğŸ“Š ê¸°ì¡´ Excel ì²­í¬ í˜•ì‹:")
    print(f"  - í•„ë“œ ìˆ˜: {len(excel_chunk)}")
    print(f"  - í•„ë“œ: {list(excel_chunk.keys())}")
    
    # í†µì¼ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    processor = ChunkProcessor(document_id="unified_test")
    
    # PDF ì²­í¬ë¥¼ í†µì¼ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    unified_pdf_chunk = processor.create_text_chunk(
        pdf_chunk["content"], 
        pdf_chunk["location"],
        pdf_chunk["metadata"]
    )
    
    # Excel ì²­í¬ë¥¼ í†µì¼ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    excel_data = {excel_chunk["column"]: excel_chunk["content"].split(": ")[1]}
    unified_excel_chunk = processor.create_excel_row_chunk(
        excel_data, 
        excel_chunk["row_index"]
    )
    
    print("\nğŸ”„ í†µì¼ëœ í˜•ì‹:")
    print(f"  - PDF ì²­í¬ í•„ë“œ: {list(unified_pdf_chunk.keys())}")
    print(f"  - Excel ì²­í¬ í•„ë“œ: {list(unified_excel_chunk.keys())}")
    
    # ê³µí†µ í•„ë“œ í™•ì¸
    pdf_fields = set(unified_pdf_chunk.keys())
    excel_fields = set(unified_excel_chunk.keys())
    common_fields = pdf_fields & excel_fields
    
    print(f"\nâœ… ê³µí†µ í•„ë“œ ({len(common_fields)}ê°œ): {sorted(common_fields)}")
    
    return unified_pdf_chunk, unified_excel_chunk


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ RAG ìµœì í™” ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    try:
        # 1. ì²­í¬ í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸
        test_chunk = test_chunk_processor()
        
        # 2. Excel í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸
        excel_chunks = test_excel_processor()
        
        # 3. í–¥ìƒëœ ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        retriever = test_enhanced_retriever()
        
        # 4. í˜•ì‹ í†µì¼ í…ŒìŠ¤íŠ¸
        unified_chunks = test_format_unification()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 60)
        
        # ê²°ê³¼ ìš”ì•½
        print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
        print(f"  âœ… ì²­í¬ í”„ë¡œì„¸ì„œ: í’ˆì§ˆ ì ìˆ˜ {test_chunk['metadata']['quality_score']:.3f}")
        print(f"  âœ… Excel í”„ë¡œì„¸ì„œ: {len(excel_chunks)}ê°œ ì²­í¬ ìƒì„±")
        print(f"  âœ… í–¥ìƒëœ ê²€ìƒ‰: ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ë²• ì§€ì›")
        print(f"  âœ… í˜•ì‹ í†µì¼: PDF/Excel ì²­í¬ í˜•ì‹ í†µí•©")
        
        print("\nğŸ’¡ RAG ìµœì í™” ì•„ì´ë””ì–´:")
        print("  1. í†µì¼ëœ ì²­í¬ í˜•ì‹ìœ¼ë¡œ ê²€ìƒ‰ ì¼ê´€ì„± í–¥ìƒ")
        print("  2. í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ê²€ìƒ‰ ìš°ì„ ìˆœìœ„ ì„¤ì •")
        print("  3. í‚¤ì›Œë“œ ì¶”ì¶œë¡œ ì •í™•í•œ ë§¤ì¹­ ê°•í™”")
        print("  4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ì˜ë¯¸ì +í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°í•©")
        print("  5. ë©”íƒ€ë°ì´í„° í™œìš©í•œ ê³ ê¸‰ í•„í„°ë§")
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 