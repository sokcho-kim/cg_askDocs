# ì‹¤ì œ ìš´ì˜ìš© RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì›Œí¬í”Œë¡œìš°
"""
ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ì²­í‚¹ ë°©ì‹ì„ ì„ íƒí•˜ì—¬
ì‹¤ì œ ìš´ì˜ìš© RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì›Œí¬í”Œë¡œìš°:
1. ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ (ìµœì  ì²­í‚¹ ë°©ì‹ ì„ íƒ)
2. ì„ íƒëœ ë°©ì‹ìœ¼ë¡œ ì²­í¬ ìƒì„±
3. ChromaDB ì¸ë±ì‹±
4. RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸
"""

import sys
import json
from pathlib import Path
from typing import Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from scripts.parse_pdf import parse_pdf_to_chunks
from scripts.parse_excel import parse_excel_file
from rag.retriever import EnhancedRetriever
from rag.chatbot import RAGChatbot


class ProductionWorkflow:
    """ì‹¤ì œ ìš´ì˜ìš© RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self, output_dir: str = "data/processed"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.best_chunking_method = None
        self.experiment_results = None
        
    def load_experiment_results(self):
        """ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ ë° ìµœì  ì²­í‚¹ ë°©ì‹ ì„ íƒ"""
        experiment_file = self.output_dir / "chunking_experiment_results.json"
        
        if not experiment_file.exists():
            print("âš ï¸ ì‹¤í—˜ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(adaptive)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.best_chunking_method = "adaptive"
            return
        
        print("ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        with open(experiment_file, 'r', encoding='utf-8') as f:
            self.experiment_results = json.load(f)
        
        # ìµœì  ì²­í‚¹ ë°©ì‹ ì„ íƒ (í‰ê·  ì ìˆ˜ ê¸°ì¤€)
        valid_methods = []
        for method, result in self.experiment_results.items():
            if "error" not in result and "rag_performance" in result:
                valid_methods.append({
                    "method": method,
                    "avg_score": result["rag_performance"]["avg_score"],
                    "avg_search_time": result["rag_performance"]["avg_search_time"],
                    "chunk_count": result["chunk_count"]
                })
        
        if valid_methods:
            # ì ìˆ˜ì™€ ì†ë„ë¥¼ ê³ ë ¤í•œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
            for method_data in valid_methods:
                # ì ìˆ˜ëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ê³ , ê²€ìƒ‰ ì‹œê°„ì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
                method_data["composite_score"] = (
                    method_data["avg_score"] * 0.7 +  # ì ìˆ˜ ê°€ì¤‘ì¹˜ 70%
                    (1.0 - min(method_data["avg_search_time"], 1.0)) * 0.3  # ì†ë„ ê°€ì¤‘ì¹˜ 30%
                )
            
            # ì¢…í•© ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ë°©ì‹ ì„ íƒ
            best_method = max(valid_methods, key=lambda x: x["composite_score"])
            self.best_chunking_method = best_method["method"]
            
            print(f"ğŸ† ìµœì  ì²­í‚¹ ë°©ì‹ ì„ íƒ: {self.best_chunking_method.upper()}")
            print(f"  â€¢ í‰ê·  ì ìˆ˜: {best_method['avg_score']:.3f}")
            print(f"  â€¢ í‰ê·  ê²€ìƒ‰ ì‹œê°„: {best_method['avg_search_time']:.3f}ì´ˆ")
            print(f"  â€¢ ì²­í¬ ìˆ˜: {best_method['chunk_count']}ê°œ")
            print(f"  â€¢ ì¢…í•© ì ìˆ˜: {best_method['composite_score']:.3f}")
        else:
            print("âš ï¸ ìœ íš¨í•œ ì‹¤í—˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’(adaptive)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.best_chunking_method = "adaptive"
    
    def generate_production_chunks(self):
        """ì„ íƒëœ ë°©ì‹ìœ¼ë¡œ ì‹¤ì œ ìš´ì˜ìš© ì²­í¬ ìƒì„±"""
        print(f"\nğŸš€ ì‹¤ì œ ìš´ì˜ìš© ì²­í¬ ìƒì„± ì‹œì‘ ({self.best_chunking_method.upper()} ë°©ì‹)")
        print("=" * 60)
        
        # PDF ì²­í¬ ìƒì„±
        pdf_file = "./data/raw/DR_ìŠ¤ë§ˆíŠ¸ì•¼ë“œê°œë¡ (ë°ëª¨ìš©).pdf"
        pdf_output = self.output_dir / "DR_ìŠ¤ë§ˆíŠ¸ì•¼ë“œê°œë¡ (ë°ëª¨ìš©)_chunks.json"
        
        print(f"ğŸ“„ PDF ì²˜ë¦¬ ì¤‘: {pdf_file}")
        pdf_chunks = parse_pdf_to_chunks(
            pdf_path=pdf_file,
            output_path=str(pdf_output),
            document_id="smart_yard_intro_production",
            chunking_method=self.best_chunking_method
        )
        print(f"  âœ“ PDF ì²­í¬ ìƒì„± ì™„ë£Œ: {len(pdf_chunks)}ê°œ")
        
        # Excel ì²­í¬ ìƒì„±
        excel_file = "./data/raw/DR_ê³µì •íšŒì˜ìë£Œ_ì¶”ì¶œë³¸(ë°ëª¨ìš©).xlsx"
        excel_output = self.output_dir / "DR_ê³µì •íšŒì˜ìë£Œ_ì¶”ì¶œë³¸(ë°ëª¨ìš©)_chunks.json"
        
        print(f"ğŸ“Š Excel ì²˜ë¦¬ ì¤‘: {excel_file}")
        excel_chunks = parse_excel_file(
            filepath=excel_file,
            output_path=str(excel_output)
        )
        print(f"  âœ“ Excel ì²­í¬ ìƒì„± ì™„ë£Œ: {len(excel_chunks)}ê°œ")
        
        return {
            "pdf_chunks": pdf_chunks,
            "excel_chunks": excel_chunks,
            "total_chunks": len(pdf_chunks) + len(excel_chunks)
        }
    
    def build_rag_system(self, chunks_info: Dict[str, Any]):
        """RAG ì‹œìŠ¤í…œ êµ¬ì¶• (ChromaDB ì¸ë±ì‹±)"""
        print(f"\nğŸ”§ RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œì‘")
        print("=" * 60)
        
        # ëª¨ë“  ì²­í¬ í•©ì¹˜ê¸°
        all_chunks = chunks_info["pdf_chunks"] + chunks_info["excel_chunks"]
        
        # ChromaDB ì¸ë±ì‹±
        retriever = EnhancedRetriever()
        retriever.clear_collection()
        
        print(f"ğŸ“¥ ChromaDB ì¸ë±ì‹± ì¤‘... ({len(all_chunks)}ê°œ ì²­í¬)")
        retriever.add_chunks(all_chunks)
        
        # í†µê³„ ì •ë³´ ì¶œë ¥
        stats = retriever.get_collection_stats()
        print(f"  âœ“ ì¸ë±ì‹± ì™„ë£Œ! í†µê³„: {stats}")
        
        return retriever
    
    def test_rag_chatbot(self, retriever: EnhancedRetriever):
        """RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ¤– RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        chatbot = RAGChatbot(retriever)
        
        test_queries = [
            "ìŠ¤ë§ˆíŠ¸ ì•¼ë“œë€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì¡°ì„  ì‚°ì—…ì˜ ì£¼ìš” ë„ì „ ê³¼ì œëŠ”?",
            "AI ê¸°ìˆ ì´ ì¡°ì„ ì†Œì—ì„œ ì–´ë–»ê²Œ í™œìš©ë˜ë‚˜ìš”?",
            "ìƒì‚°ì„± í–¥ìƒì„ ìœ„í•œ ë°©ë²•ì€?",
            "ìë™í™” ì‹œìŠ¤í…œì˜ íš¨ê³¼ëŠ”?"
        ]
        
        print("ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰:")
        for i, query in enumerate(test_queries, 1):
            print(f"\n{i}. ì¿¼ë¦¬: '{query}'")
            try:
                result = chatbot.chat(query)
                response = result['response']  # ë”•ì…”ë„ˆë¦¬ì—ì„œ response í‚¤ ì¶”ì¶œ
                print(f"   ì‘ë‹µ: {response[:200]}...")
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {e}")
        
        print(f"\nâœ… RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    def save_production_config(self, chunks_info: Dict[str, Any]):
        """ìš´ì˜ ì„¤ì • ì €ì¥"""
        config = {
            "chunking_method": self.best_chunking_method,
            "pdf_chunks_count": len(chunks_info["pdf_chunks"]),
            "excel_chunks_count": len(chunks_info["excel_chunks"]),
            "total_chunks": chunks_info["total_chunks"],
            "files": {
                "pdf_chunks": "DR_ìŠ¤ë§ˆíŠ¸ì•¼ë“œê°œë¡ (ë°ëª¨ìš©)_chunks.json",
                "excel_chunks": "DR_ê³µì •íšŒì˜ìë£Œ_ì¶”ì¶œë³¸(ë°ëª¨ìš©)_chunks.json"
            }
        }
        
        config_path = self.output_dir / "production_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ìš´ì˜ ì„¤ì • ì €ì¥: {config_path}")
        return config
    
    def run_full_workflow(self):
        """ì „ì²´ ìš´ì˜ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        print("ğŸš€ ì‹¤ì œ ìš´ì˜ìš© RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œì‘")
        print("=" * 60)
        
        # 1. ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ìµœì  ë°©ì‹ ì„ íƒ
        self.load_experiment_results()
        
        # 2. ì‹¤ì œ ìš´ì˜ìš© ì²­í¬ ìƒì„±
        chunks_info = self.generate_production_chunks()
        
        # 3. RAG ì‹œìŠ¤í…œ êµ¬ì¶•
        retriever = self.build_rag_system(chunks_info)
        
        # 4. RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸
        self.test_rag_chatbot(retriever)
        
        # 5. ìš´ì˜ ì„¤ì • ì €ì¥
        config = self.save_production_config(chunks_info)
        
        print(f"\nğŸ‰ ìš´ì˜ìš© RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“‹ êµ¬ì¶• ìš”ì•½:")
        print(f"  â€¢ ì„ íƒëœ ì²­í‚¹ ë°©ì‹: {self.best_chunking_method.upper()}")
        print(f"  â€¢ ì´ ì²­í¬ ìˆ˜: {chunks_info['total_chunks']}ê°œ")
        print(f"  â€¢ PDF ì²­í¬: {len(chunks_info['pdf_chunks'])}ê°œ")
        print(f"  â€¢ Excel ì²­í¬: {len(chunks_info['excel_chunks'])}ê°œ")
        print(f"  â€¢ ì„¤ì • íŒŒì¼: data/processed/production_config.json")
        
        return {
            "retriever": retriever,
            "config": config,
            "chunks_info": chunks_info
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    workflow = ProductionWorkflow()
    result = workflow.run_full_workflow()
    
    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. RAG ì±—ë´‡ ì‚¬ìš©: result['retriever']ë¡œ ê²€ìƒ‰")
    print(f"  2. ì„¤ì • í™•ì¸: data/processed/production_config.json")
    print(f"  3. ì²­í¬ íŒŒì¼: data/processed/DR_*_chunks.json")


if __name__ == "__main__":
    main() 