"""
RAG ì±—ë´‡ êµ¬í˜„
í–¥ìƒëœ ê²€ìƒ‰ ê¸°ëŠ¥(í•˜ì´ë¸Œë¦¬ë“œ, í’ˆì§ˆ í•„í„°ë§, ìš°ì„ ìˆœìœ„)ì„ í™œìš©í•œ ì±—ë´‡
"""

import json
import os
import requests
from typing import List, Dict, Any, Optional
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
import sys
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from rag.retriever import EnhancedRetriever

# --- smart_filter í•¨ìˆ˜ ì¶”ê°€ ---
def is_duplicate(content, seen=None):
    if seen is None:
        seen = set()
    key = content.strip()[:50]
    if key in seen:
        return True
    seen.add(key)
    return False

def smart_filter(results, score_threshold=0.4, min_length=50):
    seen = set()
    filtered = []
    for r in results:
        if (
            r.get("score", 0) >= score_threshold
            and len(r.get("content", "")) >= min_length
            and not is_duplicate(r["content"], seen)
        ):
            filtered.append(r)
    return filtered

class RAGChatbot:
    """í–¥ìƒëœ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í™œìš©í•˜ëŠ” RAG ì±—ë´‡"""
    
    def __init__(self, retriever: Optional[EnhancedRetriever] = None, 
                 gemma_api_url: Optional[str] = None, 
                 gemma_model: str = "google/gemma-3-12b-it"):
        """
        Args:
            retriever: EnhancedRetriever ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ ìë™ ìƒì„±)
            gemma_api_url: Gemma API URL (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ)
            gemma_model: ì‚¬ìš©í•  Gemma ëª¨ë¸ëª…
        """
        self.retriever = retriever or EnhancedRetriever()
        self.conversation_history = []
        
        # Gemma API ì„¤ì •
        self.gemma_model = gemma_model
        if gemma_api_url:
            self.gemma_api_url = gemma_api_url
        else:
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ API URL ë¡œë“œ
            self.gemma_api_url = os.getenv('GEMMA_API_URL')
            if not self.gemma_api_url:
                print("Warning: GEMMA_API_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                self.gemma_api_url = None
        
        # API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.llm_available = self.gemma_api_url is not None
        
    def search_context(self, query: str, search_method: str = "hybrid", **kwargs) -> List[Dict[str, Any]]:
        """
        ì¿¼ë¦¬ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            search_method: ê²€ìƒ‰ ë°©ë²•
                - "hybrid": í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ê¸°ë³¸)
                - "semantic": ì˜ë¯¸ì  ê²€ìƒ‰ë§Œ
                - "keyword": í‚¤ì›Œë“œ ê²€ìƒ‰ë§Œ
                - "quality": í’ˆì§ˆ í•„í„°ë§ ê²€ìƒ‰
                - "priority": ìš°ì„ ìˆœìœ„ ê²€ìƒ‰
                - "enhanced": í–¥ìƒëœ ê²€ìƒ‰ (ì—‘ì…€ ìš°ì„  + í•˜ì´ë¸Œë¦¬ë“œ)
            **kwargs: ê²€ìƒ‰ íŒŒë¼ë¯¸í„° (n_results, min_quality_score ë“±)
        
        Returns:
            ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        n_results = kwargs.get('n_results', 10)
        
        if search_method == "enhanced":
            return self._enhanced_search(query, n_results)
        elif search_method == "hybrid":
            return self.retriever.hybrid_search(query, n_results)
        elif search_method == "semantic":
            return self.retriever.semantic_search(query, n_results)
        elif search_method == "keyword":
            return self.retriever.keyword_search(query, n_results)
        elif search_method == "quality":
            min_quality = kwargs.get('min_quality_score', 0.5)
            return self.retriever.quality_filtered_search(query, n_results, min_quality)
        elif search_method == "priority":
            return self.retriever.search_by_priority(query, n_results)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²€ìƒ‰ ë°©ë²•: {search_method}")
    
    def _enhanced_search(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """í–¥ìƒëœ ê²€ìƒ‰: ì—‘ì…€ ë°ì´í„° ìš°ì„  + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        
        # 1. í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ì—‘ì…€ ë°ì´í„° ìš°ì„  ì°¾ê¸°
        keyword_results = self.retriever.keyword_search(query, n_results * 2)
        
        # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ì „ì²´ ê²€ìƒ‰
        hybrid_results = self.retriever.hybrid_search(query, n_results * 2)
        
        # 3. ê²°ê³¼ í†µí•© ë° ì •ë ¬
        combined_results = {}
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ (ì—‘ì…€ ìš°ì„ ) - ê°€ì¤‘ì¹˜ ë†’ê²Œ
        for result in keyword_results:
            chunk_id = result['chunk_id']
            if chunk_id not in combined_results:
                combined_results[chunk_id] = {
                    'chunk_id': chunk_id,
                    'content': result['content'],
                    'metadata': result['metadata'],
                    'score': result.get('score', 0) * 2.0,  # ì—‘ì…€ ë°ì´í„° ê°€ì¤‘ì¹˜ ëŒ€í­ ì¦ê°€
                    'source': 'keyword'
                }
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (í‚¤ì›Œë“œ ê²°ê³¼ê°€ ì—†ì„ ë•Œë§Œ)
        for result in hybrid_results:
            chunk_id = result['chunk_id']
            if chunk_id not in combined_results:  # í‚¤ì›Œë“œ ê²°ê³¼ê°€ ì—†ì„ ë•Œë§Œ ì¶”ê°€
                combined_results[chunk_id] = {
                    'chunk_id': chunk_id,
                    'content': result['content'],
                    'metadata': result['metadata'],
                    'score': result.get('final_score', result.get('score', 0)),
                    'source': 'hybrid'
                }
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_results = sorted(
            combined_results.values(),
            key=lambda x: x['score'],
            reverse=True
        )
        
        return sorted_results[:n_results]
    
    def format_search_results(self, contexts: List[Dict[str, Any]]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
        if not contexts:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_results = []
        for i, context in enumerate(contexts, 1):
            chunk_id = context.get('chunk_id', 'unknown')
            content = context.get('content', '')
            score = context.get('final_score', context.get('score', 0))
            metadata = context.get('metadata', {})
            
            # ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            document_id = chunk_id.split('_')[0] if '_' in chunk_id else chunk_id
            location = metadata.get('location', 'unknown')
            title = metadata.get('title', '')
            
            # ê²°ê³¼ í¬ë§·íŒ…
            result_info = f"[{i}] ğŸ“„ ë¬¸ì„œ: {document_id}"
            if location != 'unknown':
                result_info += f" | ğŸ“ ìœ„ì¹˜: {location}"
            if title:
                result_info += f" | ğŸ“ ì œëª©: {title}"
            result_info += f" | â­ ì ìˆ˜: {score:.3f}\n"
            result_info += f"ğŸ’¬ ë‚´ìš©: {content}"
            
            formatted_results.append(result_info)
        
        return "\n\n".join(formatted_results)
    
    def generate_response(self, query: str, contexts: List[Dict[str, Any]]) -> str:
        """LLM(Gemma) ê¸°ë°˜ ë‹µë³€ ìƒì„±. smart_filterë¡œ ì¶”ë¦° ì»¨í…ìŠ¤íŠ¸ë¥¼ LLM í”„ë¡¬í”„íŠ¸ë¡œ ì „ë‹¬."""
        filtered = smart_filter(contexts)
        if not filtered:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ğŸ¤– ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”. ì§ˆë¬¸ì„ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”!"
        if self.llm_available:
            try:
                prompt = self._build_llm_prompt(query, filtered)
                response = self._call_gemma_api(prompt)
                if response:
                    return response
            except Exception as e:
                print(f"Gemma API í˜¸ì¶œ ì‹¤íŒ¨, ëŒ€ì²´ ë‹µë³€ ì‚¬ìš©: {e}")
        # LLM ì‹¤íŒ¨ ì‹œ fallback
        return self._generate_simple_response(query, filtered)

    
    def _build_llm_prompt(self, query: str, contexts: List[Dict[str, Any]]) -> str:
        """LLMìš© í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not contexts:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        context_strs = []
        for i, ctx in enumerate(contexts, 1):
            chunk_id = ctx.get('chunk_id', 'unknown')
            content = ctx.get('content', '')
            metadata = ctx.get('metadata', {})
            
            # ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            document_id = chunk_id.split('_')[0] if '_' in chunk_id else chunk_id
            location = metadata.get('location', 'unknown')
            title = metadata.get('title', '')
            
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ êµ¬ì„±
            context_info = f"[{i}] ë¬¸ì„œ: {document_id}"
            if location != 'unknown':
                context_info += f" | ìœ„ì¹˜: {location}"
            if title:
                context_info += f" | ì œëª©: {title}"
            
            context_strs.append(f"{context_info}\në‚´ìš©: {content}")
        
        context_block = "\n\n".join(context_strs)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì¹œì ˆí•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í•œêµ­ì–´ AI ì±—ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ìš”ì•½ì…ë‹ˆë‹¤:

---
{context_block}
---

ğŸ§  ì‚¬ìš©ì ì§ˆë¬¸: {query}

âœï¸ ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ë“¯ ë‹µë³€í•˜ì„¸ìš”.  
   (ì˜ˆ: "~ì— ëŒ€í•´ ì•Œë ¤ë“œë¦´ê²Œìš”.", "~ë¼ëŠ” íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤.")
2. **ê° ì •ë³´ëŠ” ì¤„ë°”ê¿ˆ(ì—”í„°)**ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì‘ì„±í•˜ì„¸ìš”.  
   â†’ ë¬¸ë‹¨ ì—†ì´ **í•œ ë¬¸ì¥ë‹¹ í•œ ì¤„**ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
3. í•„ìš”í•œ ê³³ì— ì ì ˆí•œ **ì´ëª¨ì§€(ì˜ˆ: âœ…, ğŸ“Œ, ğŸ”§)**ë¥¼ ë¬¸ë‹¨ ì²˜ìŒì— ì‚¬ìš©í•˜ì„¸ìš”.
4. ë§ˆì§€ë§‰ ì¤„ì—ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ **ì¶œì²˜**ë¥¼ ë°˜ë“œì‹œ ì¶”ê°€í•˜ì„¸ìš”:

ğŸ“š [ì¶œì²˜] ë¬¸ì„œëª…, ìœ„ì¹˜ ë˜ëŠ” í˜ì´ì§€ ì •ë³´

ğŸ“ ì˜ˆì‹œ ì¶œë ¥ í˜•ì‹:
ìŠ¤ë§ˆíŠ¸ ì•¼ë“œì™€ ê¸°ì¡´ ì¡°ì„ ì†Œì˜ ì°¨ì´ë¥¼ ì•Œë ¤ë“œë¦´ê²Œìš”. 
ìŠ¤ë§ˆíŠ¸ ì•¼ë“œëŠ” ìë™í™” ìˆ˜ì¤€ì´ ë†’ì•„ ì‘ì—… íš¨ìœ¨ì´ í–¥ìƒë©ë‹ˆë‹¤.  
ğŸ“¡ ì‹¤ì‹œê°„ ì •ë³´ ê³µìœ  ì²´ê³„ë¡œ ë¶€ì„œ ê°„ í˜‘ì—…ì´ ì‰¬ì›Œì§‘ë‹ˆë‹¤.
ğŸ§  ì§€ëŠ¥í˜• ì˜ì‚¬ê²°ì • ì‹œìŠ¤í…œì´ ì ìš©ë˜ì–´ íŒë‹¨ ì†ë„ê°€ ë¹¨ë¼ì§‘ë‹ˆë‹¤. 
ğŸ“š [ì¶œì²˜] ìŠ¤ë§ˆíŠ¸ì•¼ë“œ_ê¸°ìˆ ë³´ê³ ì„œ.pdf, page 7

---
ì´ì œ ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.

ğŸ“ ë‹µë³€:
"""

        return prompt
    
    def _call_gemma_api(self, prompt: str) -> str:
        """Gemma APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.gemma_api_url:
            raise Exception("Gemma API URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        headers = {
            "accept": "application/json",
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "model": self.gemma_model,
            "stream": False
        }
        
        try:
            response = requests.post(self.gemma_api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content'].strip()
            else:
                print(f"API ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {result}")
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"Gemma API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
        except Exception as e:
            print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return None
    
    def _generate_simple_response(self, query: str, contexts: List[Dict[str, Any]]) -> str:
        """ë‹¨ìˆœí•œ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        if not contexts:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ìƒìœ„ 3ê°œ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
        top_contexts = contexts[:3]
        
        response = f"ì§ˆë¬¸: {query}\n\n"
        response += "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
        
        for i, ctx in enumerate(top_contexts, 1):
            chunk_id = ctx.get('chunk_id', 'unknown')
            content = ctx.get('content', '')
            metadata = ctx.get('metadata', {})
            
            # ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ
            document_id = chunk_id.split('_')[0] if '_' in chunk_id else chunk_id
            location = metadata.get('location', 'unknown')
            
            response += f"[{i}] ë¬¸ì„œ: {document_id}"
            if location != 'unknown':
                response += f" | ìœ„ì¹˜: {location}"
            response += f"\në‚´ìš©: {content[:300]}...\n\n"
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€
        sources = set()
        for ctx in top_contexts:
            chunk_id = ctx.get('chunk_id', 'unknown')
            document_id = chunk_id.split('_')[0] if '_' in chunk_id else chunk_id
            sources.add(document_id)
        
        response += f"ì°¸ê³  ì •ë³´:\nì¶œì²˜: {', '.join(sources)}\n\n"
        response += "ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."
        
        return response
    
    def chat(self, query: str, search_method: str = "enhanced", **kwargs) -> Dict[str, Any]:
        """
        ì±—ë´‡ê³¼ ëŒ€í™”í•©ë‹ˆë‹¤.
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            search_method: ê²€ìƒ‰ ë°©ë²•
            **kwargs: ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
        
        Returns:
            ì‘ë‹µ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        # 1. ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        contexts = self.search_context(query, search_method, **kwargs)
        
        # 2. ì‘ë‹µ ìƒì„±
        response = self.generate_response(query, contexts)
        
        # 3. ëŒ€í™” ê¸°ë¡ ì €ì¥
        conversation_entry = {
            "query": query,
            "search_method": search_method,
            "contexts_found": len(contexts),
            "response": response,
            "contexts": contexts
        }
        self.conversation_history.append(conversation_entry)
        
        return {
            "query": query,
            "response": response,
            "contexts_found": len(contexts),
            "search_method": search_method,
            "contexts": contexts
        }
    
    def compare_search_methods(self, query: str) -> Dict[str, Any]:
        """ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ë²•ì„ ë¹„êµí•©ë‹ˆë‹¤."""
        methods = ["hybrid", "semantic", "keyword", "quality", "priority"]
        results = {}
        
        for method in methods:
            try:
                contexts = self.search_context(query, method, n_results=3)
                results[method] = {
                    "contexts_found": len(contexts),
                    "avg_score": sum(ctx.get('final_score', ctx.get('score', 0)) for ctx in contexts) / len(contexts) if contexts else 0,
                    "top_context": contexts[0] if contexts else None
                }
            except Exception as e:
                results[method] = {"error": str(e)}
        
        return results
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """ëŒ€í™” ê¸°ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.conversation_history
    
    def clear_history(self):
        """ëŒ€í™” ê¸°ë¡ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.conversation_history = []


def test_rag_chatbot():
    """RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸"""
    print("ğŸ¤– RAG ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # ì±—ë´‡ ì´ˆê¸°í™”
    chatbot = RAGChatbot()
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_queries = [
        "ìŠ¤ë§ˆíŠ¸ ì•¼ë“œë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ìë™í™” ê¸°ìˆ ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
        "AI ê¸°ìˆ ì´ ì–´ë–»ê²Œ í™œìš©ë˜ë‚˜ìš”?",
        "ì¡°ì„ ì†Œì˜ ìƒì‚°ì„± í–¥ìƒ ë°©ë²•ì€?",
        "ìŠ¤ë§ˆíŠ¸ ì•¼ë“œì˜ ì£¼ìš” ê¸°ìˆ  ìš”ì†ŒëŠ”?"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ” ì§ˆë¬¸ {i}: {query}")
        print("-" * 40)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ì‘ë‹µ
        result = chatbot.chat(query, search_method="hybrid", n_results=3)
        
        print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {result['contexts_found']}ê°œ ì»¨í…ìŠ¤íŠ¸")
        print(f"ğŸ¤– ì‘ë‹µ: {result['response']}")
        
        # ìƒìœ„ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        if result['contexts']:
            top_context = result['contexts'][0]
            print(f"ğŸ† ìµœê³  ì ìˆ˜ ì»¨í…ìŠ¤íŠ¸: {top_context.get('chunk_id', 'unknown')} (ì ìˆ˜: {top_context.get('final_score', 0):.3f})")
    
    # ê²€ìƒ‰ ë°©ë²• ë¹„êµ
    print(f"\nğŸ” ê²€ìƒ‰ ë°©ë²• ë¹„êµ: 'ìŠ¤ë§ˆíŠ¸ ì•¼ë“œ ìë™í™”'")
    print("=" * 50)
    
    comparison = chatbot.compare_search_methods("ìŠ¤ë§ˆíŠ¸ ì•¼ë“œ ìë™í™”")
    for method, result in comparison.items():
        if "error" in result:
            print(f"  {method.upper()}: âŒ {result['error']}")
        else:
            print(f"  {method.upper()}: {result['contexts_found']}ê°œ ì»¨í…ìŠ¤íŠ¸, í‰ê·  ì ìˆ˜ {result['avg_score']:.3f}")
    
    # ëŒ€í™” ê¸°ë¡ ìš”ì•½
    history = chatbot.get_conversation_history()
    print(f"\nğŸ“‹ ëŒ€í™” ê¸°ë¡: {len(history)}ê°œ ì§ˆë¬¸")
    
    return chatbot


if __name__ == "__main__":
    chatbot = test_rag_chatbot()
