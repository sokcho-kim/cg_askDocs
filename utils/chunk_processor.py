"""
ë¬¸ì„œ ì²­í¬ ë¶„í•  ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ ê³µí†µ í´ë˜ìŠ¤
PDF, Excel, ê¸°íƒ€ ë¬¸ì„œ íƒ€ì…ì— ëŒ€í•´ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬ ì²˜ë¦¬ ë¡œì§
"""

import json
import uuid
import re
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from pathlib import Path


class ChunkProcessor(ABC):
    """ë¬¸ì„œ ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, document_id: Optional[str] = None, max_chunk_size: int = 1000, overlap: int = 100):
        """
        Args:
            document_id: ë¬¸ì„œ ê³ ìœ  ID (Noneì´ë©´ ìë™ ìƒì„±)
            max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
            overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© í¬ê¸° (ë¬¸ì ìˆ˜)
        """
        self.document_id = document_id or str(uuid.uuid4())
        self.max_chunk_size = max_chunk_size
        self.overlap = overlap
        self.chunk_index = 0
        self.chunks: List[Dict[str, Any]] = []
    
    def split_text_to_chunks(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        if len(text) <= self.max_chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.max_chunk_size
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
            if end < len(text):
                # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ, ì¤„ë°”ê¿ˆ ë’¤ì—ì„œ ìë¥´ê¸°
                for punct in ['.', '!', '?', '\n']:
                    last_punct = text.rfind(punct, start, end)
                    if last_punct > start:
                        end = last_punct + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - self.overlap
            if start >= len(text):
                break
        
        return chunks
    
    def extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (RAG ê²€ìƒ‰ ìµœì í™”ìš©)"""
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë¡œ êµ¬ì„±ëœ ë‹¨ì–´ ì¶”ì¶œ
        keywords = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text)
        
        # ê¸¸ì´ê°€ 2 ì´ìƒì¸ ë‹¨ì–´ë§Œ í•„í„°ë§
        keywords = [kw for kw in keywords if len(kw) >= 2]
        
        # ë¹ˆë„ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ (ìµœëŒ€ 10ê°œ)
        keyword_freq = {}
        for kw in keywords:
            keyword_freq[kw] = keyword_freq.get(kw, 0) + 1
        
        # ë¹ˆë„ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)
        return [kw for kw, freq in sorted_keywords[:10]]
    
    def calculate_chunk_quality_score(self, content: str, chunk_type: str) -> float:
        """ì²­í¬ì˜ í’ˆì§ˆ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (RAG ê²€ìƒ‰ ìš°ì„ ìˆœìœ„ìš©)"""
        score = 0.0
        
        # ê¸°ë³¸ ì ìˆ˜: ê¸¸ì´ ê¸°ë°˜
        if len(content) > 50:
            score += 0.3
        elif len(content) > 20:
            score += 0.2
        else:
            score += 0.1
        
        # í‚¤ì›Œë“œ ë‹¤ì–‘ì„± ì ìˆ˜
        keywords = self.extract_keywords(content)
        if len(keywords) >= 5:
            score += 0.3
        elif len(keywords) >= 3:
            score += 0.2
        else:
            score += 0.1
        
        # ì²­í¬ íƒ€ì…ë³„ ì ìˆ˜
        if chunk_type == "table":
            score += 0.2  # êµ¬ì¡°í™”ëœ ë°ì´í„°ëŠ” ë†’ì€ ì ìˆ˜
        elif chunk_type == "text":
            score += 0.1
        elif chunk_type == "image":
            score += 0.15
        
        # íŠ¹ìˆ˜ íŒ¨í„´ ì ìˆ˜
        if re.search(r'[0-9]+', content):  # ìˆ«ì í¬í•¨
            score += 0.1
        if re.search(r'[ê°€-í£]+', content):  # í•œê¸€ í¬í•¨
            score += 0.1
        
        return min(score, 1.0)  # ìµœëŒ€ 1.0
    
    def create_standard_chunk(self, 
                            content: str, 
                            chunk_type: str = "text", 
                            location: str = "", 
                            metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """í†µì¼ëœ í˜•ì‹ì˜ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (RAG ìµœì í™”ìš©)"""
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extract_keywords(content)
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = self.calculate_chunk_quality_score(content, chunk_type)
        
        chunk = {
            "chunk_id": f"{self.document_id}_{chunk_type}_{self.chunk_index}",
            "document_id": self.document_id,
            "chunk_index": self.chunk_index,
            "chunk_type": chunk_type,
            "location": location,
            "content": content,
            "embedding": None,
            "metadata": {
                "length": len(content),
                "chunk_in_content": 0,
                "keywords": keywords,
                "quality_score": quality_score,
                "search_priority": "high" if quality_score > 0.7 else "medium" if quality_score > 0.4 else "low",
                **(metadata or {})
            }
        }
        self.chunk_index += 1
        return chunk
    
    def create_text_chunk(self, content: str, location: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return self.create_standard_chunk(content, "text", location, metadata)
    
    def create_image_chunk(self, caption: str, location: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return self.create_standard_chunk(caption, "image", location, metadata)
    
    def create_table_chunk(self, content: str, location: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """í‘œ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return self.create_standard_chunk(content, "table", location, metadata)
    
    def create_excel_row_chunk(self, row_data: Dict[str, str], row_index: int, sheet_name: str = "Sheet1") -> Dict[str, Any]:
        """Excel í–‰ ë°ì´í„°ë¥¼ í†µì¼ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        # í–‰ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        content_parts = []
        for column, value in row_data.items():
            if value and str(value).strip():
                content_parts.append(f"{column}: {value}")
        
        content = " | ".join(content_parts)
        location = f"sheet:{sheet_name},row:{row_index}"
        
        metadata = {
            "row_index": row_index,
            "sheet_name": sheet_name,
            "columns": list(row_data.keys()),
            "data_type": "excel_row"
        }
        
        return self.create_standard_chunk(content, "table", location, metadata)
    
    def add_chunk(self, chunk: Dict[str, Any]):
        """ì²­í¬ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
        self.chunks.append(chunk)
    
    def process_text_content(self, text: str, location: str, metadata: Optional[Dict[str, Any]] = None):
        """í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        if not text.strip():
            return
        
        text_chunks = self.split_text_to_chunks(text)
        for i, chunk_text in enumerate(text_chunks):
            chunk_metadata = {
                "chunk_in_content": i,
                **(metadata or {})
            }
            chunk = self.create_text_chunk(chunk_text, location, chunk_metadata)
            self.add_chunk(chunk)
    
    def filter_high_quality_chunks(self, min_quality_score: float = 0.5) -> List[Dict[str, Any]]:
        """í’ˆì§ˆ ì ìˆ˜ê°€ ë†’ì€ ì²­í¬ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤."""
        return [chunk for chunk in self.chunks 
                if chunk.get('metadata', {}).get('quality_score', 0) >= min_quality_score]
    
    def sort_chunks_by_quality(self) -> List[Dict[str, Any]]:
        """ì²­í¬ë¥¼ í’ˆì§ˆ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤."""
        return sorted(self.chunks, 
                     key=lambda x: x.get('metadata', {}).get('quality_score', 0), 
                     reverse=True)
    
    def save_chunks(self, output_path: str):
        """ì²­í¬ë“¤ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        
        print(f"[âœ“] ì²­í¬ ì €ì¥ ì™„ë£Œ: {len(self.chunks)}ê°œ ì²­í¬ -> {output_path}")
        
        # í’ˆì§ˆ í†µê³„ ì¶œë ¥
        quality_scores = [chunk.get('metadata', {}).get('quality_score', 0) for chunk in self.chunks]
        if quality_scores:
            avg_quality = sum(quality_scores) / len(quality_scores)
            high_quality_count = len([s for s in quality_scores if s > 0.7])
            print(f"[ğŸ“Š] í’ˆì§ˆ í†µê³„: í‰ê·  {avg_quality:.2f}, ê³ í’ˆì§ˆ ì²­í¬ {high_quality_count}ê°œ")
    
    def get_chunks(self) -> List[Dict[str, Any]]:
        """ì²˜ë¦¬ëœ ì²­í¬ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.chunks
    
    def process(self, document) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ê°ì²´(AbstractDocument)ë¥¼ ë°›ì•„ ì²­í¬ ë¶„í• """
        self.chunks = []
        self.chunk_index = 0
        # ê¸°ë³¸ì ìœ¼ë¡œ get_pages()ë¥¼ ì‚¬ìš© (PDF, Excel ë“±)
        if hasattr(document, 'get_pages'):
            for page_info in document.get_pages():
                location = f"page:{page_info.get('page_num', '?')}"
                # í…ìŠ¤íŠ¸ ì²­í¬
                if page_info.get("text"):
                    self.process_text_content(page_info["text"], location, {"page": page_info.get("page_num")})
                # ì´ë¯¸ì§€ ì²­í¬ (xrefë§Œ ì œê³µ, ì‹¤ì œ ì´ë¯¸ì§€ëŠ” í•„ìš”ì‹œ ì¶”ì¶œ)
                for img in page_info.get("images", []):
                    metadata = {
                        "page": page_info.get("page_num"),
                        "image_index": img[0] if isinstance(img, (list, tuple)) else None
                    }
                    chunk = self.create_image_chunk(f"[Image xref {img[0]}]", location, metadata)
                    self.add_chunk(chunk)
        elif hasattr(document, 'get_chunks'):
            # ì´ë¯¸ ì²­í¬í™”ëœ ë¬¸ì„œ ê°ì²´ ì§€ì›
            for chunk in document.get_chunks():
                self.add_chunk(chunk)
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ ê°ì²´ì…ë‹ˆë‹¤.")
        return self.chunks


class PDFChunkProcessor(ChunkProcessor):
    """PDF ë¬¸ì„œ ì „ìš© ì²­í¬ í”„ë¡œì„¸ì„œ (ì´ì œ ë¬¸ì„œ ê°ì²´ë§Œ ë°›ìŒ)"""
    
    def process_pdf_by_blocks(self, document) -> List[Dict[str, Any]]:
        """ë¸”ë¡ ë‹¨ìœ„ë¡œ PDF ì²­í‚¹ (ë” ì„¸ë°€í•œ ë¶„í• )"""
        self.chunks = []
        self.chunk_index = 0
        
        if hasattr(document, 'get_blocks'):
            for block_info in document.get_blocks():
                location = f"page:{block_info.get('page_num', '?')},block:{block_info.get('block_num', '?')}"
                
                if block_info.get("type") == "text":
                    self.process_text_content(
                        block_info["text"], 
                        location, 
                        {
                            "page": block_info.get("page_num"),
                            "block": block_info.get("block_num"),
                            "bbox": block_info.get("bbox"),
                            "chunking_method": "block"
                        }
                    )
                elif block_info.get("type") == "image":
                    metadata = {
                        "page": block_info.get("page_num"),
                        "block": block_info.get("block_num"),
                        "bbox": block_info.get("bbox"),
                        "chunking_method": "block"
                    }
                    chunk = self.create_image_chunk(f"[Image block {block_info.get('block_num')}]", location, metadata)
                    self.add_chunk(chunk)
        
        return self.chunks
    
    def process_pdf_by_sections(self, document) -> List[Dict[str, Any]]:
        """ì„¹ì…˜ ë‹¨ìœ„ë¡œ PDF ì²­í‚¹ (ì œëª© ê¸°ë°˜ ë¶„í• )"""
        self.chunks = []
        self.chunk_index = 0
        
        if hasattr(document, 'get_sections'):
            for section_info in document.get_sections():
                location = f"page:{section_info.get('page_num', '?')},section:{section_info.get('title', '?')}"
                
                # ì„¹ì…˜ ì œëª©ì„ í¬í•¨í•œ ì „ì²´ ë‚´ìš©
                full_content = f"{section_info.get('title', '')}\n{section_info.get('content', '')}"
                
                self.process_text_content(
                    full_content,
                    location,
                    {
                        "page": section_info.get("page_num"),
                        "title": section_info.get("title"),
                        "chunking_method": "section"
                    }
                )
        
        return self.chunks
    
    def process_pdf_by_pages(self, document) -> List[Dict[str, Any]]:
        """í˜ì´ì§€ ë‹¨ìœ„ë¡œ PDF ì²­í‚¹ (ê¸°ë³¸ ë°©ì‹)"""
        return self.process(document)
    
    def process_pdf_adaptive(self, document, max_chunk_size: int = 800) -> List[Dict[str, Any]]:
        """ì ì‘í˜• PDF ì²­í‚¹ (ë‚´ìš©ì— ë”°ë¼ ìë™ ì„ íƒ)"""
        self.chunks = []
        self.chunk_index = 0
        
        if hasattr(document, 'get_pages'):
            for page_info in document.get_pages():
                page_text = page_info.get("text", "")
                page_num = page_info.get("page_num", 0)
                
                # í˜ì´ì§€ í¬ê¸°ì— ë”°ë¼ ì²­í‚¹ ë°©ì‹ ê²°ì •
                if len(page_text) <= max_chunk_size:
                    # ì‘ì€ í˜ì´ì§€: í˜ì´ì§€ ë‹¨ìœ„
                    location = f"page:{page_num}"
                    self.process_text_content(page_text, location, {"page": page_num, "chunking_method": "page"})
                else:
                    # í° í˜ì´ì§€: ë¸”ë¡ ë‹¨ìœ„ë¡œ ì¬ì‹œë„
                    if hasattr(document, 'get_blocks'):
                        for block_info in document.get_blocks():
                            if block_info.get("page_num") == page_num:
                                location = f"page:{page_num},block:{block_info.get('block_num', '?')}"
                                
                                if block_info.get("type") == "text":
                                    self.process_text_content(
                                        block_info["text"],
                                        location,
                                        {
                                            "page": page_num,
                                            "block": block_info.get("block_num"),
                                            "chunking_method": "adaptive_block"
                                        }
                                    )
                
                # ì´ë¯¸ì§€ ì²˜ë¦¬
                for img in page_info.get("images", []):
                    metadata = {
                        "page": page_num,
                        "image_index": img[0] if isinstance(img, (list, tuple)) else None,
                        "chunking_method": "adaptive"
                    }
                    chunk = self.create_image_chunk(f"[Image xref {img[0]}]", f"page:{page_num}", metadata)
                    self.add_chunk(chunk)
        
        return self.chunks


class ExcelChunkProcessor(ChunkProcessor):
    """Excel ë¬¸ì„œ ì „ìš© ì²­í¬ í”„ë¡œì„¸ì„œ (ì´ì œ ë¬¸ì„œ ê°ì²´ë§Œ ë°›ìŒ)"""
    
    def process_excel_data(self, df_data: List[Dict[str, Any]], sheet_name: str = "Sheet1") -> List[Dict[str, Any]]:
        """Excel ë°ì´í„°í”„ë ˆì„ì„ í†µì¼ëœ ì²­í¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        self.chunks = []
        self.chunk_index = 0
        
        for row_index, row_data in enumerate(df_data):
            # ê° í–‰ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë³€í™˜
            chunk = self.create_excel_row_chunk(row_data, row_index, sheet_name)
            self.add_chunk(chunk)
        
        return self.chunks 